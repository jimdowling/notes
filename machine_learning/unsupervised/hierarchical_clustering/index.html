<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Hierarchical Clustering" />
<meta property="og:description" content="It&rsquo;s not wildly off base to remark that a dendrogram, the visual result of Hierarchical Clustering, looks sort of like a Decision Tree, but in reverse.
(Pulled from Google Images)
from IPython.display import Image Image(&#39;images/dendrogram.PNG&#39;) But whereas the Decision Tree starts from all points collected together and making successive splits to separate the data, Hierarchical Clustering starts with all disjoint points and iteratively finds groupings of similar points.
Algorithm Actually performing Hierarchical Clustering all begins with some measure of &ldquo;dissimilarity." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/unsupervised/hierarchical_clustering/" />



<meta property="article:published_time" content="2019-10-02T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2019-10-02T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hierarchical Clustering"/>
<meta name="twitter:description" content="It&rsquo;s not wildly off base to remark that a dendrogram, the visual result of Hierarchical Clustering, looks sort of like a Decision Tree, but in reverse.
(Pulled from Google Images)
from IPython.display import Image Image(&#39;images/dendrogram.PNG&#39;) But whereas the Decision Tree starts from all points collected together and making successive splits to separate the data, Hierarchical Clustering starts with all disjoint points and iteratively finds groupings of similar points.
Algorithm Actually performing Hierarchical Clustering all begins with some measure of &ldquo;dissimilarity."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Hierarchical Clustering",
  "url": "https://napsterinblue.github.io/notes/machine_learning/unsupervised/hierarchical_clustering/",
  "wordCount": "473",
  "datePublished": "2019-10-02T00:00:00&#43;00:00",
  "dateModified": "2019-10-02T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Hierarchical Clustering</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Hierarchical Clustering</h1>
    <div class="technical_note_date">
      <time datetime=" 2019-10-02T00:00:00Z "> 02 Oct 2019</time>
    </div>
  </header>
  <div class="content">
  

<p>It&rsquo;s not wildly off base to remark that a <em>dendrogram</em>, the visual result of Hierarchical Clustering, looks sort of like a Decision Tree, but in reverse.</p>

<p>(Pulled from Google Images)</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/dendrogram.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="hierarchical_clustering_2_0.png" alt="png" /></p>

<p>But whereas the Decision Tree starts from <em>all points collected together</em> and making successive splits to separate the data, Hierarchical Clustering starts with <em>all disjoint points</em> and iteratively finds groupings of similar points.</p>

<h2 id="algorithm">Algorithm</h2>

<p>Actually performing Hierarchical Clustering all begins with some measure of &ldquo;dissimilarity.&rdquo; For the dataset below, we might consider the pairwise Euclidean Distance from point to point</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/fig_10_8.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="hierarchical_clustering_6_0.png" alt="png" /></p>

<p>Then we do the following (per ISL) until we&rsquo;ve blobbed together every point:</p>

<pre><code>1. Make each point its own unique cluster

2. While len(clusters) &gt; 1:

    a. Examine all inter-cluster, pairwise dissimilarities
    b. Find the smallest value between two clusters
    c. Fuse them into one cluster
    d. Compute the new pairwise, inter-cluster dissimilarities
</code></pre>

<p>And if you&rsquo;ve got a dissimlarity measure picked out, the first step of this (for all <code>n</code> distinct points) is pretty trivial. However, when you graduate to multi-point clusters, it requires more thought.</p>

<p>From here, step <code>2d</code> requires you to pick a <em>linkage function</em> for handling cluster vs cluster calculation. Summarizing the popular methods, we have:</p>

<ul>
<li>Complete: Compute all pairwise dissimilarities and take the max.</li>
<li>Single: Compute all pairwise dissimilarities and take the min.</li>
<li>Average: Compute all pairwise dissimilarities and take the average.</li>
<li>Centroid: Average the points in each cluster, <em>then</em> compute the pairwise dissimilarity</li>
</ul>

<p>Complete and Average are said to make for more balanced trees than Single, as shown below.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/fig_10_12.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="hierarchical_clustering_8_0.png" alt="png" /></p>

<p>Finally, we can draw a horizontal line at any of various levels along the dendrogram to arrive at <code>m</code> different groups. The closer two terminal points are to one another, vertically, the more similar they are.</p>

<p><strong>Note</strong>: There is <em>no</em> concept of horizontal similarity when looking at these visuals. It&rsquo;s merely a product of how the graphics are built.</p>

<h2 id="considerations">Considerations</h2>

<h3 id="dissimilarity-measure">Dissimilarity Measure</h3>

<p>In the example above, we used Euclidean Distance as our measure for dissimilarity. However, ISL presents a compelling example to consider other statistics.</p>

<p>Take a hypothetical Amazon dataset, for instance. If each row is a user of the site and each column represents the quantity of purchases for a given item, then we wind up with a <em>very</em> sparse dataset. Working with other clustering algorithms such as K-Means or KNN, we know that this leads quickly to the curse of dimensionality.</p>

<p>Suppose instead, we used Correlation between two clients. This could afford insight into overlaps of cross-product preferences in a way that simple distance cannot.</p>

<h3 id="scaling-features">Scaling Features</h3>

<p>Extending this shopping example, the following 3 charts represent the same two features, but scaled differently</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/fig_10_14.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="hierarchical_clustering_10_0.png" alt="png" /></p>

<p>As you can see, feature normalization can have a huge impact on how your algorithm churns through your data (the last panel is count * cost)</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 179 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
