{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Decision Tree Basics\"\n",
    "date: 2019-09-25\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of Decision Trees isn't hard to grok-- indeed following the flow-chart of decision criteria is why they're considered one of the more powerful algorithms from an interpretability standpoint.\n",
    "\n",
    "We'll cover actually building the tree below. So skipping ahead, after we've constructed our tree we wind up with `m` neat rectangles in some many-dimensional space. It's important to note that when we're done, every single data point-- trained or new-- maps directly onto one of these `m` rectangles.\n",
    "\n",
    "Finally, from a prediction standpoint, when we get a new `X`, we trace through the tree and figure out which rectangle it lands in, then make the prediction:\n",
    "\n",
    "- Classification: Pick the class with the most representation in the rectangle/terminal node\n",
    "- Regression: Return the average value of `y` for the training data that generated this rectangle/terminal node\n",
    "\n",
    "## Building Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we construct our Decision Tree, we only split on one variable at a time, per layer. Overall, our goal is to maximize the *information gain* at each split.\n",
    "\n",
    "Thus at each stage, it looks roughly like\n",
    "\n",
    "``` python\n",
    "for feature in features:\n",
    "    if is_continuous(feature):\n",
    "        # actual split value generation varies by implementation\n",
    "        # e.g. height (in) might try every 5 in between min and max\n",
    "        lots_of_splits = get_splits(data[feature])\n",
    "        \n",
    "        best_split, best_loss_value = check_continuous_splits(\n",
    "                                         data, feature, lots_of_splits)\n",
    "    \n",
    "    elif is_categorical(feature):\n",
    "        best_split = []\n",
    "        best_loss_value = []\n",
    "        \n",
    "        # for (A, B, C): A vs B+C, B vs A+C, C vs A+B\n",
    "        checks = generate_one_vs_many(data[feature])\n",
    "        \n",
    "        for check in checks:\n",
    "            loss_value = calculate_loss(split_on_check(data, feature))\n",
    "            if loss_value < best_loss_value:\n",
    "                best_split = check\n",
    "                best_loss_value = loss_value\n",
    "                \n",
    "new_tree = tree.split_on(best_split)\n",
    "\n",
    "# recursively do this step until you've organized your tree\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to this notion of \"make a bunch of multi-dimensional rectangles\", we can define half-planes `R_1` and `R_2` as\n",
    "\n",
    "$R_1(j, s): \\{X|X<s\\}$\n",
    "\n",
    "$R_2(j, s): \\{X|X\\geq s\\}$\n",
    "\n",
    "That account for all points that fall within `R` for feature `j`, split at the value `s`\n",
    "\n",
    "### Objective Functions\n",
    "\n",
    "The Objective Function that you're trying to minimize throughout Tree construction depends, obviously, on whether or not you're working on a Classification or Regression problem.\n",
    "\n",
    "The Regression form should look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANQAAAAzCAYAAADvnyYiAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAApQSURBVHhe7Zy/a9vOG8f9Z2g1ZAl0aLZ4rKFDDB1iyPA1ZAiegulQTIdisgSToZgMwXyGIDoUnKHgDAFn+ICzFNwh4AwFZwg4QwcNGTR40JDh/X2e0538I7Il2bKt9HMvOEgkObmT7v38upNT0Gg0saEFpdHEiBaURhMjWlAaTYxoQWk0MaIFpdHEiBaURhMjWlAaTYxoQWk0MaIFpdHEyHoFNbDRvygilUpRK6L5JI+P4PysYIvPbxRh3lmwB/KERpNA1u6hnH/LUlBp1G7lQcWgg8pbPpdC+qQrD2o0yWXtguqeGDAMQ4imeGnLo4yDzlGGzrGgDJSuR89pNMvGRudrAZty/m1+KKP1KE/NYM2C6sN8byC/m3O90NehF+JQL7tfQF54rzwaf+QJjWYFdE9zKF9Z4mfnTwtljpS26+iJI9OJJCj7ugxDTPD5WvWX/EMKu4WSQbnTdc3Nk/abEEOgUK+6Q8ev5PEQA5mL5x7qO+N9jNT2Gm5/NbHSvyiIeWbsN8jkroMuah+b5KOG9M+z9MyDDXtED0Vh2HFmZFLlUL1qo33j31oXdZT30t71LwRFXii1Y6LPwuJrjCoNhf9HjsI/C/3vefE547gjP7AEHhsoCLfuts1D03csbmvCPCoiq65PsKCsmxpKp3w3XxfObRUZCrFyn8rI0X3O0BiSgHVBc5HmZ9BMjB7ysffYHk7ADE322Q9tKMJJQXVPt2SYRxZBFB/yqF9UkTtgT2WjdcjHlp8/KYsoxmQU0AiKlZUIEykoR4Qr2cDnkkDE3MqgdCX90mMTJfq9+msJI7EojCPj2Qs1tRy0P6eQOw/2l/PlUHc1siJyAtKEL/wImFZPTRRfCMpCY89A5YZvltth8fc2KNQTE5purvAEq8ifWLxuYUS0bfKUAc+w+3UrkYJi45BeW6i0CA56Ny10Hidu/KCPDkVBvWVYBxbsh1JwseHBRD7kPZ27KBHNqvfR2J0QlMqf5Ix0Y9Q0ij9kt0m0S82fJonqeTlcTZqghKHLoHYnf9cE4vyiEJMMaGfa+ibPi70Z5yeYW1BCJPujVr1GgdsMBjacZ/7BgW330aY8iRdzG4/ucV6PEknoM523emh+Jg/Af/d9DR2LrhF/ZLmIm6vGw6Hm1ax4gMexil6FRXpZXSiJCEdHBjInPvnmoAfz04SYnNnPfAFBEU/sZdQEDJNPJZ/u6UjRJUw+lRRkWL2VkCT+VUGG1HiRWpDDOCzAvLXJcMp230DxqD1zji8mKMLbGiSagfK/r11SdCP31HiocRVSnkkyaseJfz5ro/utivJ+Fmke0/uRMf2uu145KMJYOmvso6wyZ72iw0T05bWtwHB6YUGxyxwrpb8mqz6NiVL6a/C8bg7qszQxhqqm5mCqZzSgYx948vjvpVw96+hj1y2A7S5ezIlBUIRDHRpJ6P+GON76MVJ0SaVR/ZlsSXWO3b7OFhRdd+ReV/kpDzBcYBFLFS7WXRu9wInLVVp1f8K08NXaMH2MFzWWSuA6UxDxCIq5ryMrbtyS1g1WjoPWR/fBrm/FPjxhBcVrf3xd/rsaERcz6Jmpjcmi+ppC+lOLzszCgXXrt/g9rXVhhZwWgX2MHSWoxZdoYhKUCvsMFC6SPvVCosK+WSXVBKF2lQQJSqz403Wbct8k70zIHi7L8s9HqD6OvvqzUYJ5WUfx3SayB1W0HqIa9IQJShUm1p5rOBbaZyXkDhYNOWVhImo+aHXR/j3vrg5eLuig9p4frIH8SQONLzlsbudROusEeAtCVKrIql8EjJyuE5PwS5t+6cPcyQ9zlYc2zLMyiu9p3A/y2DqY1ccR7KuSuM74KL2pWrt8G7V4kaSQLymW/E8Hrd8d1HlCLpTDzettKSz5SInzRhmteTXlJeQZ1H/z7zaaB/x7KmBNjHDaKPPkIqM2E3peYgf/ronWeQ65f8aXzTvHNIYQe9aWSkAfFSrXyn2Tz+mP/NzIhoFwJKUooYoRiansSUuzgKBUMSIub8ubVIvbww3Cw+YTXqiJZFTQlv9c5Uajr7ZMQ2yHCrLOaiMyt506emKxXeEK2rP462JmHxXK+KSHuZX0UEZUYcj/F8ca3gKCUpZ8C5UwFbCH4aKY/dBE9X9ZbG7QxLlvofyOrWIJrScb3bM80hzyeAlpFBYUlPK29Png/07jPynAvKcfn/ton9dRPsiiMNJvsT3LyKF6Y4USpxfCqNdYPA+VDnePxUK72h85jQ6qPFH9jKC08J7FXxsz+qjwvBHNG1K/89hChYx7eifci4CjODcVMqI5mDGEuXMLSu3lC2vJ+99yYwtnJoVmxg65850qGiQinkiZdwVUrhuo0M/GSddLTme2sRBnAUGpvXxhvS2FWBWj7HkSN4cxhkUB8cBDGhuJCLdoTJkzCnEcMi7nLMhNFM7Cv4YhnsvM8LtHYTGFsz4GyxW0CjfXyfQ+KtRCtrFbhSm2saWwOeVrEuzLIlJvpnlu12j5bj2ag/kEJUOT0OXkAcX3ZPm9XRTKpcvJ65ZJDWRPSJxWEwX6md+His68giJvc8R9CF/y71NsP7qaL8YgrSXDoePYan8grpFhb1T6ZsotXYWIuYBL//u0HeduVMFG0C+k468j4DHULxvorG2Rd3YfFaKvnvFh48b3a3j/J3EG/s+VhZl+F1/+H11Q3q5sd2Ort8/Jp1n3HbQva8hv8PXZoUvlRToWEFlenki8Ez3F4R8PSpyb1/3OJyjP236lh+gzjmHro3vTgvkpK65PfVb7uuT/HdmmxB5ZeNCpbSKHEoaEjktRqs/7x/WUP2yQ0ZlRqLBv6yixgWJrL0LqAsrUbyE035zELaoYbwqo386ayssgbB8VQ+Oj8idh4PzuF++y2PG/j869ieIeea4Yi2kRBWWh6bvHKUwbliQ9a85WUHor9WamOLfBb+7Sf1tByOe+ITrxN0M2L89TY2BrKRFx+Vsac8iH5YUw3tcAuF59qtV9duTu/SBstL+wAUgjd9yCFeozqyZiH70IZ1i8Gd6vSa/uFi9e7DGlnL5y2o79fkQS1Ng7UFGbV3mZmPTCIylL454zDmtoXIRYe3lBREFNvAMVtXlbY0iUnD+V/mmi8Uv12t1gaXyooj350pwPanfAaEHAs7oTuYF9ybnO9PDm72X01R9+BvL1H3lW3S9jv46uClkfTGRjWLANS3hBPXXRuvbbUhKyqQVPuV6irPl47uGGMmG/smkcutl3plule8ufD3qHyn1D1LevoVoHfel97OuSVzwYn+O8g7qM/Kyy+YBC47sGSqIEnEKWwk5LeTXP6mZQuex74xHFg0j52X8Xca8iL/TOz3xFCc0acb8uYKlfXPPXsPp7pQX16nBzgsCdE/91HC4itVE1tla6WVsL6rWx4pzgtdI7400HaeQohF6dnLSgXhWObbu7yiOvs2lWhRbUa4FLxQblA2/U16xpkogWlEYTI1pQGk2MaEFpNDGiBaXRxIgWlEYTI1pQGk2MaEFpNDGiBaXRxIgWlEYTI1pQGk1sAP8HjuS4z6wAFWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('images/regression_cost.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, on the other hand, can take a few different forms.\n",
    "\n",
    "Because our predictions are based on the majority class in any terminal node, our loss/objective functions will all be based on `pHat_mk`, the proportion of the majority class `k` in node `m`. However, because I already know LaTeX will throw a fit if I try and put `mk` in the underscore, we'll refer to this variable as `rho`.\n",
    "\n",
    "And so the most basic function we could come up with is the *error rate*, the probability that we see another class in this terminal node.\n",
    "\n",
    "$E = 1 - max_k(\\rho)$\n",
    "\n",
    "#### Advanced Classification Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most popular loss functions for classification introduce more sensitivity to the diversity of class representation in each node.\n",
    "\n",
    "To illustrate that, let's construct a few simple hypothetical nodes, `proportions`, of *decreasing* diversity, and examine their effects on the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "proportions = np.array(\n",
    "               [[0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "                [0.1, 0.0, 0.0, 0.0, 0.9],\n",
    "                [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "                [0.33, 0.33, 0.33, 0.0, 0.0],\n",
    "                [0.25, 0.25, 0.25, 0.0, 0.25],\n",
    "                [0.2, 0.2, 0.2, 0.2, 0.2]])\n",
    "\n",
    "# make zero vals basically zero for clean log-work\n",
    "proportions = np.clip(proportions, 1e-12, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Gini Index* (the default loss function in `sklearn`) is defined by\n",
    "\n",
    "$G = \\sum_{k=1}^{K} \\rho(1 - \\rho)$\n",
    "\n",
    "Throwing our dummy data at it, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000\n",
      "0.18000\n",
      "0.50000\n",
      "0.66330\n",
      "0.75000\n",
      "0.80000\n"
     ]
    }
   ],
   "source": [
    "for prop in proportions:\n",
    "    gini = np.sum([(x * (1 - x)) for x in prop])\n",
    "    print(f'{gini:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the equation, it should be a no-brainer why our first `proportions` list yielded a straight-zero score, and increased as we had less-clear separation.\n",
    "\n",
    "**Note**: The Gini Index is more-commonly referred to as the *node purity*\n",
    "\n",
    "Another popular loss function is [*Categorical Cross Entropy*](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), which should be second-nature to anyone doing multiclass Deep Learning.\n",
    "\n",
    "Again, plugging our data in yields a clear increase in values as our dataset becomes less separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00000\n",
      "0.32508\n",
      "0.69315\n",
      "1.09758\n",
      "1.38629\n",
      "1.60944\n"
     ]
    }
   ],
   "source": [
    "for prop in proportions:\n",
    "    entropy = np.sum([-(x * np.log(x)) for x in prop])\n",
    "    print(f'{entropy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, if the construction of our trees is done in a recursive fashion, we need some sort of break condition.\n",
    "\n",
    "There are a number of things that we might consider in this regard. Drawing from the docstring of `sklearn.tree.DecisionTreeClassifier` we can get a brief rundown of our options and their defaults\n",
    "\n",
    "```\n",
    "max_depth : int or None, optional (default=None)\n",
    "    The maximum depth of the tree. If None, then nodes are expanded until\n",
    "    all leaves are pure or until all leaves contain less than\n",
    "    min_samples_split samples.\n",
    "\n",
    "min_samples_split : int, float, optional (default=2)\n",
    "    The minimum number of samples required to split an internal node:\n",
    "\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "    - If float, then `min_samples_split` is a fraction and\n",
    "      `ceil(min_samples_split * n_samples)` are the minimum\n",
    "      number of samples for each split.\n",
    "\n",
    "min_samples_leaf : int, float, optional (default=1)\n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    A split point at any depth will only be considered if it leaves at\n",
    "    least ``min_samples_leaf`` training samples in each of the left and\n",
    "    right branches.  This may have the effect of smoothing the model,\n",
    "    especially in regression.\n",
    "\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "    - If float, then `min_samples_leaf` is a fraction and\n",
    "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "      number of samples for each node.\n",
    "\n",
    "max_features : int, float, string or None, optional (default=None)\n",
    "    The number of features to consider when looking for the best split:\n",
    "\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "\n",
    "max_leaf_nodes : int or None, optional (default=None)\n",
    "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "\n",
    "min_impurity_decrease : float, optional (default=0.)\n",
    "    A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value.\n",
    "```\n",
    "\n",
    "In general, though it's important to note that setting most of these to `None` and hitting Go will amount to this Tree perfectly overfitting our data-- a trait obviously undesirable in any model.\n",
    "\n",
    "Few options here:\n",
    "\n",
    "- Intelligently apply constraints via Cross Validation/Grid Searching\n",
    "- [Prune our over-fit trees](https://napsterinblue.github.io/notes/machine_learning/trees/decision_tree_pruning/)\n",
    "- Move onto Tree-based ensemble methods\n",
    "\n",
    "On that last point, we consider the tradeoff between model performance and interpretability-- we can either have a neat flow-chart of decision making, or a potentially-vague \"variable importance\" score across arbitrarily-many models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
