{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Boosted Models\"\n",
    "date: 2019-09-29\n",
    "type: technical_note\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of Boosting a given model extends, intuitively, from two other Data Science concepts:\n",
    "\n",
    "- [Principal Component Analysis](https://napsterinblue.github.io/notes/stats/techniques/pca/), which aims to find an axis that explains the most variation, re-orient your original data, then find a new axis that explains what the first couldnt, etc\n",
    "- [Bootstrapping](https://napsterinblue.github.io/notes/machine_learning/validation/bootstrap/), which involves maximizing the use of a dataset by repeated sampling with replacement and aggregating model generation \n",
    "\n",
    "Essentially, want to make simple model that predicts on `y`. Then we subtract these predictions from `y` to get a bunch of residuals that we missed. Then we train a second model on these residuals. We subtract what we learned from *these* residuals and rinse-repeat.\n",
    "\n",
    "At each step, we tack the model we just trained to the end of a big ol' list of models.\n",
    "\n",
    "When we're done, predictions on new `X` values will be fed into each of these models, and our output will be weighted by a constant `lambda` for each model.\n",
    "\n",
    "## With Tree-Based Models\n",
    "\n",
    "Model Boosting was introduced to me in the context of Decision Trees in Chapter 8 of ISL, but the idea extends neatly-enough to other models.\n",
    "\n",
    "The algorithm that they outline looks like the following:\n",
    "\n",
    "**Note**: We start off making a simple Decision Tree to predict on `y`, then we predict on updated residuals every step thereafter, so we call `r=y` at the start.\n",
    "\n",
    "--------------\n",
    "\n",
    "```python\n",
    "trees = []\n",
    "r = y     # residuals are just the first y values\n",
    "\n",
    "for b in range(1, B):\n",
    "    f_b = DecisionTree(terminal_nodes=d).fit(X, r)\n",
    "    \n",
    "    trees.append(f_b)\n",
    "    \n",
    "    r = r - lambda_ * f_b(x)\n",
    "    \n",
    "f_x = lambda_ * np.sum([tree(x) for tree in trees])\n",
    "\n",
    "```\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Shrinkage\n",
    "\n",
    "The *shrinkage parameter* only makes sense to have values between `0` and `1`, but in practice we'll use something closer to `0.01` or `0.001`. It's whole job is to slow down the learning process from model to model.\n",
    "\n",
    "Look at the residual update step above and consider `lambda` values small and large:\n",
    "\n",
    "- If small, then the residuals are only a *little* different than they were in the last step and we learn *almost* the same thing\n",
    "- If large, then we're basically sculpting with dynamite, haphazardly over-correcting at each step\n",
    "\n",
    "#### Number of Models\n",
    "\n",
    "It's not hard to imagine that a very large *Model Count*, `B`, will lead to over-fitting. Infinately-many models trained to reduce the prediction error of the last model will collapse on perfect interpolation at the cost of flexibility.\n",
    "\n",
    "This is typically a value that we arrive at via cross-validation.\n",
    "\n",
    "**Note**: There is some tricky interplay between this and our `lambda` value-- a small shrinkage parameter will require a lot of aggregated models to arrive at the appropriate convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
