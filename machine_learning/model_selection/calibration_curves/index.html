<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Using Calibration Curves to Pick Your Classifier" />
<meta property="og:description" content="Motivation Very similar to our discussion on using QQ plots to check the Normality of your data, Calibration Curves are used to check the quantile relationship between your predictions and the underlying values they try to predict.
So what does that mean?
Recall that nearly all implementations of our Classifiers actually output a probability under the hood, which is compared against some threshold to make our decisions. Now imagine sorting all of those probability values, smallest to largest." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/model_selection/calibration_curves/" />



<meta property="article:published_time" content="2020-10-09T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2020-10-09T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using Calibration Curves to Pick Your Classifier"/>
<meta name="twitter:description" content="Motivation Very similar to our discussion on using QQ plots to check the Normality of your data, Calibration Curves are used to check the quantile relationship between your predictions and the underlying values they try to predict.
So what does that mean?
Recall that nearly all implementations of our Classifiers actually output a probability under the hood, which is compared against some threshold to make our decisions. Now imagine sorting all of those probability values, smallest to largest."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Using Calibration Curves to Pick Your Classifier",
  "url": "https://napsterinblue.github.io/notes/machine_learning/model_selection/calibration_curves/",
  "wordCount": "1018",
  "datePublished": "2020-10-09T00:00:00&#43;00:00",
  "dateModified": "2020-10-09T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Using Calibration Curves to Pick Your Classifier</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Using Calibration Curves to Pick Your Classifier</h1>
    <div class="technical_note_date">
      <time datetime=" 2020-10-09T00:00:00Z "> 09 Oct 2020</time>
    </div>
  </header>
  <div class="content">
  

<h2 id="motivation">Motivation</h2>

<p>Very similar to <a href="https://napsterinblue.github.io/notes/stats/basics/qq_plots/">our discussion on using QQ plots to check the Normality of your data</a>, Calibration Curves are used to check the quantile relationship between your predictions and the underlying values they try to predict.</p>

<p>So what does that mean?</p>

<p>Recall that nearly all implementations of our Classifiers actually output a <em>probability</em> under the hood, which is compared against some threshold to make our decisions. Now imagine sorting all of those probability values, smallest to largest. If our model was &ldquo;well-calibrated&rdquo;, we should expect to see some reliable patterns when we examine these sorted values, by quantile.</p>

<p>Concretely, if we look at records found in the first 10% of probabilities, the average predicted probability <em>should</em> be consistent with the percent of positive records in this sample. Similarly for the 10-20th percentile, and the 20-30th, etc, etc.</p>

<h2 id="in-sklearn">In sklearn</h2>

<p><code>sklearn</code> has a helpful utility function, <code>calibration_curve()</code>, that allows us to examine these relationships quickly. The code is largely lifted from <a href="https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#sphx-glr-auto-examples-calibration-plot-compare-calibration-py">their docs</a>, with my own commentary.</p>

<p>For the sake of example, let&rsquo;s generate a fake dataset comprised of 10k records and of the 20 features, 2 are useful and 2 aren&rsquo;t.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">filterwarnings</span>
<span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span></code></pre></div>
<pre><code>Populating the interactive namespace from numpy and matplotlib
</code></pre>

<p>As you can see, the breakout of <code>0/1</code> values is very nearly <sup>50</sup>&frasl;<sub>50</sub></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">Counter</span><span class="p">(</span><span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>Counter({1: 5008, 0: 4992})
</code></pre>

<h3 id="simple-case">Simple Case</h3>

<p>And so splitting out the data like we usually do</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=.</span><span class="mi">7</span><span class="p">,</span>
                                                    <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span></code></pre></div>
<p>We&rsquo;ll fit a Logistic Regression to our training data</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span></code></pre></div>
<p>Which yields an average predicted value just a bit south of <sup>50</sup>&frasl;<sub>50</sub>, which is ideal because it lines up with our <code>0/1</code> distribution</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code></pre></div>
<pre><code>0.49228571428571427
</code></pre>

<p>Now we&rsquo;ll express our predictions in terms of <code>Pr(True)</code> and feed it into <code>calibration_curve()</code> along with the correct values of <code>y_train</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>

<span class="n">n_bins</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">prob_true</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># results are tuples of (Pr(False), Pr(True))</span>
<span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">prob_true</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">)</span></code></pre></div>
<p>This <code>n_bins</code> argument is important, because it determines how granularly we examine the quantiles to check &ldquo;average prediction&rdquo; vs &ldquo;proportion of True records.&rdquo;</p>

<p>Under the hood, the function defaults to creating uniform bins between <code>0</code> and <code>1</code></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">n_bins</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span></code></pre></div>
<pre><code>[0.         0.1        0.2        0.3        0.4        0.5
 0.60000001 0.70000001 0.80000001 0.90000001 1.00000001]
</code></pre>

<p>And so as a sanity check, we&rsquo;ll use these bins to interrogate the returned values of <code>fraction_of_positives</code>.</p>

<p>The first bin is in the range <code>(0, 0.1]</code>, so we&rsquo;ll filter down all of our probability predictions within that band. Then use that index to filter down our <code>y_train</code> records. Finally, taking the mean of what&rsquo;s left gives us the proportion of <code>True</code> records in the first quantile.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="p">[</span><span class="n">prob_true</span> <span class="o">&lt;=</span> <span class="o">.</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code></pre></div>
<pre><code>0.054071661237785014
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fraction_of_positives</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code></pre></div>
<pre><code>0.054071661237785014
</code></pre>

<p>The same relationship holds for the next bin</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="p">[(</span><span class="mf">0.1</span> <span class="o">&lt;</span> <span class="n">prob_true</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">prob_true</span> <span class="o">&lt;=</span> <span class="mf">0.2</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code></pre></div>
<pre><code>0.1181959564541213
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fraction_of_positives</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></code></pre></div>
<pre><code>0.1181959564541213
</code></pre>

<p>and so on&hellip;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fraction_of_positives</span></code></pre></div>
<pre><code>array([0.05407166, 0.11819596, 0.21372549, 0.33595801, 0.41899441,
       0.51815981, 0.66287016, 0.78496241, 0.87447699, 0.94272727])
</code></pre>

<p>Similarly, we want to look at these same cuts, and see <em>what our average predicted value is</em>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">mean_predicted_value</span></code></pre></div>
<pre><code>array([0.04023254, 0.14607538, 0.24808382, 0.34888665, 0.4520183 ,
       0.54961305, 0.65147235, 0.75068661, 0.8530717 , 0.94677561])
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">prob_true</span><span class="p">[</span><span class="n">prob_true</span> <span class="o">&lt;=</span> <span class="mf">0.1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code></pre></div>
<pre><code>0.040232536676567954
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">prob_true</span><span class="p">[(</span><span class="mf">0.1</span> <span class="o">&lt;</span> <span class="n">prob_true</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">prob_true</span> <span class="o">&lt;=</span> <span class="mf">0.2</span><span class="p">)]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></code></pre></div>
<pre><code>0.14607538336214732
</code></pre>

<h3 id="creating-the-plots">Creating the Plots</h3>

<p>And so taking these 10 quantile observations and plotting them against one another, we can see a roughly-linear relationship forming. The orange, dashed line represents the perfect relationship where our predictions line up 1-1 with the proportion of positive records.</p>

<p>Couple caveats here, we should expect a Logistic Regression to produce a pretty straight line&ndash; after all, the underling mechanism is a linear one.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">prob_true</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Average Predicted Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion of Positive Records&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_28_0.png" alt="png" /></p>

<p>However, increasing the number of bins from 10 to 50 reveals that intra-bin errors were more or less cancelling each other out.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">prob_true</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Average Predicted Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion of Positive Records&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_30_0.png" alt="png" /></p>

<p>Indeed, when we look at a histogram of predicted probabilities, the &ldquo;above the dotted line&rdquo; bit between <code>(0, 0.05]</code> makes sense, as our model has a bias of predicting values very close to zero.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">prob_true</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_32_0.png" alt="png" /></p>

<p>Overall, despite being well-calibrated (hugging the dotted line pretty nicely), our model has room for improvements by most measures of accuracy.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.classification</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span></code></pre></div>
<pre><code>array([[1189,  249],
       [ 277, 1285]], dtype=int64)
</code></pre>

<p>Compare that to a Random Forest approach trained on the same data</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
<span class="n">rfc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>

<span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">mean_predicted_value</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">prob_true</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Average Predicted Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion of Positive Records&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_36_0.png" alt="png" /></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_predicted_value</span><span class="p">,</span> <span class="n">fraction_of_positives</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Average Predicted Value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion of Positive Records&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_37_0.png" alt="png" /></p>

<p>Looking at the histogram of predicted probabilities, the majority of our values are very close to <code>0</code> or <code>1</code>. This should track, because tree-based methods don&rsquo;t really have a clean notion of &ldquo;continuum.&rdquo;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">prob_true</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">);</span></code></pre></div>
<p><img src="calibration_curves_39_0.png" alt="png" /></p>

<p>Instead, it more-accurately sorts the data than its Logistic counterpart</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rfc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span></code></pre></div>
<pre><code>array([[1294,  144],
       [ 180, 1382]], dtype=int64)
</code></pre>

<p>Which brings us to the conversation of when to use this tool.</p>

<h2 id="interpretation">Interpretation</h2>

<p>I came across Calibration Plots reading the book <a href="https://github.com/hundredblocks/ml-powered-applications/blob/master/ml_editor/model_evaluation.py#L153">Building ML Powered Applications</a>, and in the example the author put forward, we wanted to create a feedback loop between model and user, such that they could reliably &ldquo;hill climb&rdquo; to a high score by making minor edits to their inputs.</p>

<p>In the image below, the author compares and contrasts three such models.</p>

<p>Not pictured here, but from an accuracy standpoint the hierarchy of models was 2 &gt; 1 &gt; 3. Nevertheless, they were most interested in going with Model 3, because the product they were building was more focused on model interpretability than accuracy measures.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/calibration.png&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="calibration_curves_43_0.png" alt="png" /></p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 179 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
