<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Basics" />
<meta property="og:description" content="from IPython.display import Image Overview GANs have been in the Machine Learning spotlight since the seminal paper by Ian Goodfellow et al in 2014.
The general idea is that you&rsquo;ve got a bunch of data that you want to learn &ldquo;the essence of,&rdquo; and you make yourself two networks:
 A Generator, G, that learns the underlying data generation distribution of our sample data (e.g. how you might express every R, G, B value in terms of some probability distribution), then uses random noise to generate new data A Discriminator, D, that learns to tell the difference between real data and data that the Generator passed  Then training just becomes:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/gans/basics/" />



<meta property="article:published_time" content="2019-07-08T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2019-07-08T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Basics"/>
<meta name="twitter:description" content="from IPython.display import Image Overview GANs have been in the Machine Learning spotlight since the seminal paper by Ian Goodfellow et al in 2014.
The general idea is that you&rsquo;ve got a bunch of data that you want to learn &ldquo;the essence of,&rdquo; and you make yourself two networks:
 A Generator, G, that learns the underlying data generation distribution of our sample data (e.g. how you might express every R, G, B value in terms of some probability distribution), then uses random noise to generate new data A Discriminator, D, that learns to tell the difference between real data and data that the Generator passed  Then training just becomes:"/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Basics",
  "url": "https://napsterinblue.github.io/notes/machine_learning/gans/basics/",
  "wordCount": "623",
  "datePublished": "2019-07-08T00:00:00&#43;00:00",
  "dateModified": "2019-07-08T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Basics</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Basics</h1>
    <div class="technical_note_date">
      <time datetime=" 2019-07-08T00:00:00Z "> 08 Jul 2019</time>
    </div>
  </header>
  <div class="content">
  

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span></code></pre></div>
<h2 id="overview">Overview</h2>

<p>GANs have been in the Machine Learning spotlight since the <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">seminal paper by Ian Goodfellow et al in 2014</a>.</p>

<p>The general idea is that you&rsquo;ve got a bunch of data that you want to learn &ldquo;the essence of,&rdquo; and you make yourself two networks:</p>

<ul>
<li>A Generator, <code>G</code>, that learns the underlying data generation distribution of our sample data (e.g. how you might express every R, G, B value in terms of some probability distribution), then uses random noise to generate new data</li>
<li>A Discriminator, <code>D</code>, that learns to tell the difference between real data and data that the Generator passed</li>
</ul>

<p>Then training just becomes:</p>

<ul>
<li>The Generator makes <code>N</code> fake images using its current understanding of <code>pr_data</code>, the probability distribution that defines our images</li>
<li>It then sends <code>N</code> fake images and <code>N</code> real images over to the Discriminator</li>

<li><p>The Discriminator tries to sort real from fake</p>

<ul>
<li>Then updating based on how good/bad it did
<br /></li>
</ul></li>

<li><p>The Discriminator then passes what it learned back to the Generator</p></li>

<li><p>The Generator then learns:</p>

<ul>
<li>More about the underlying data distribution</li>
<li>How to make more convincing fakes (thus get caught less)
<br /></li>
</ul></li>
</ul>

<p>Rinse, repeat, for awhile, then eventually, your Generator will be able to make novel images that perfectly represent the underlying <code>pr_data</code> distribution</p>

<h2 id="important-math-notes">Important Math Notes</h2>

<h3 id="loss-function">Loss Function</h3>

<p>The loss function can be defined as</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/loss_fn.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="basics_7_0.png" alt="png" /></p>

<p>This looks involved, but basically you can look at each Expectation term as one step of the training process.</p>

<ul>
<li><p>Maximizing <code>log(D(x))</code> is the same as saying <strong>&ldquo;maximize the likelihood that <code>D</code> will correctly pick when the image it&rsquo;s looking at came from the actual dataset, x&rdquo;</strong></p></li>

<li><p>By extension, <code>D(G(z))</code> is <code>D</code>&rsquo;s ability to pick correctly when <code>G</code> takes some noise and generates an image. Thus, taking 1 minus that and minimizing means &ldquo;minimize the likelihood that <code>D</code> will correctly pick when <code>G</code> gives it a fake data point.&rdquo;</p>

<ul>
<li>However, for both cognitive and computational ease, we&rsquo;ll instead <em>maximize</em> <code>D(G(z))</code>, or <strong>&ldquo;maximize the likelihood of <code>D</code> picking incorrectly when it sees fake images generated by <code>G</code>&ldquo;</strong>.
<br /></li>
</ul></li>
</ul>

<p>These two terms are counter to one another, hence the &ldquo;Adversarial&rdquo; in Generative Adversarial Networks. As a consequence of this, these are notoriously difficult to train&ndash; if one gets a sharp edge over the other, it stays pinned and learned all but dies out.</p>

<h3 id="convergence">Convergence</h3>

<p>The abstract of Goodfellow&rsquo;s paper states (and he later goes on to prove)</p>

<blockquote>
<p>In the space of arbitrary functions <code>G</code> and <code>D</code>, a unique solution exists, with <code>G</code> recovering the training data distribution and <code>D</code> equal to <sup>1</sup>&frasl;<sub>2</sub> everywhere</p>
</blockquote>

<p>Two things to glean from this:</p>

<p>1) The Generator network, <code>G</code> aims to approximate the data generation distribution via very-high-dimensional <a href="https://napsterinblue.github.io/notes/stats/techniques/mle/">Maximum Likelihood Estimation</a></p>

<p>2) The long-run convergence of the most optimal GAN has the Discriminator, <code>D</code> differentiating real from fake data about 50% of the time (otherwise, <code>G</code> isn&rsquo;t doing its job very well)</p>

<p>Visually, this looks like:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/visually.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="basics_13_0.png" alt="png" /></p>

<p>For simplicity&rsquo;s sake, let&rsquo;s assume that we can represent our actual dataset with some sort of normal distribution (the dotted black line)</p>

<p>a) Our Generator isn&rsquo;t very good at first, so it samples some noise, <code>z</code> from a uniform distribution, and generates a data distribution that we can represent with the <strong>green line</strong>. The <strong>blue</strong> line is the distribution that our Discriminator uses to tell real (black) from fake (green)</p>

<p>b) We then train our Discriminator to get a little better to tell the difference between the two, smoothing out its line</p>

<p>c) The Generator, in turn, takes this information and gets a little better at approximating the probability distribution of the real data (the black line)</p>

<p>d) And this process repeats again and again until <code>pr_data</code> = <code>pr_generated</code> and the Discriminator&rsquo;s best guess is a coin flip between the two</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 179 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
