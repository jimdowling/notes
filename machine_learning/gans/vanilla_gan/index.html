<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Constructing a Vanilla GAN" />
<meta property="og:description" content="from IPython.display import Image Implementing Following along with Chollet&rsquo;s example in his Deep Learning book, he builds a GAN over a bunch of frog images from CIFAR. It&rsquo;s a great, succinct bit of code, so I&rsquo;m not going to copy-paste it all here. Instead, I&rsquo;ll comment on the things that stand out as non-obvious and important section by section
Building the Generator  Everything starts off by defining the generator_input relative to some latent_dim&ndash; the dimension of our latent space that we&rsquo;re going to use to generate noise." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://napsterinblue.github.io/notes/machine_learning/gans/vanilla_gan/" />



<meta property="article:published_time" content="2019-07-08T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2019-07-08T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Constructing a Vanilla GAN"/>
<meta name="twitter:description" content="from IPython.display import Image Implementing Following along with Chollet&rsquo;s example in his Deep Learning book, he builds a GAN over a bunch of frog images from CIFAR. It&rsquo;s a great, succinct bit of code, so I&rsquo;m not going to copy-paste it all here. Instead, I&rsquo;ll comment on the things that stand out as non-obvious and important section by section
Building the Generator  Everything starts off by defining the generator_input relative to some latent_dim&ndash; the dimension of our latent space that we&rsquo;re going to use to generate noise."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Constructing a Vanilla GAN",
  "url": "https://napsterinblue.github.io/notes/machine_learning/gans/vanilla_gan/",
  "wordCount": "868",
  "datePublished": "2019-07-08T00:00:00&#43;00:00",
  "dateModified": "2019-07-08T00:00:00&#43;00:00",
  "author": {
    "@type": "Person",
    "name": ""
  }
}
</script> 

    <title>Constructing a Vanilla GAN</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://napsterinblue.github.io/notes/css/custom.css" rel="stylesheet">
    <link href="https://napsterinblue.github.io/notes/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Data Science Notes" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://napsterinblue.github.io">Movies, Metrics, Musings</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://napsterinblue.github.io/pages/about.html" title="About">About</a></li>
                    <li><a href="https://napsterinblue.github.io/archives.html" title="Archive">Archive</a></li>
                    <li><a href="https://napsterinblue.github.io/pages/resources.html" title="Resources">Resources</a></li>
                    <li><a href="https://napsterinblue.github.io/notes/" title="Notes">My Notes</a></li>

                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
    <h1 class="technical_note_title">Constructing a Vanilla GAN</h1>
    <div class="technical_note_date">
      <time datetime=" 2019-07-08T00:00:00Z "> 08 Jul 2019</time>
    </div>
  </header>
  <div class="content">
  

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span></code></pre></div>
<h2 id="implementing">Implementing</h2>

<p>Following along with <a href="https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.5-introduction-to-gans.ipynb">Chollet&rsquo;s example in his Deep Learning book</a>, he builds a GAN over a bunch of frog images from CIFAR. It&rsquo;s a great, succinct bit of code, so I&rsquo;m not going to copy-paste it all here. Instead, I&rsquo;ll comment on the things that stand out as non-obvious and important section by section</p>

<h3 id="building-the-generator">Building the Generator</h3>

<ul>
<li><p>Everything starts off by defining the <code>generator_input</code> relative to some <code>latent_dim</code>&ndash; the dimension of our latent space that we&rsquo;re going to use to generate noise. He defines this to be 32, I&rsquo;ve seen it as high as 100-200 for larger image applications</p></li>

<li><p>The first hidden layer is a large Dense layer that is comprised of <code>(smaller height, smaller width, lots of filters)</code>. This learns to generate pixel-level representations from our latent space. We reshape this into an actual image before doing any convolution.</p></li>

<li><p>Each layer of our Generator (and later with the Discriminator) leverages a <code>LeakyReLU()</code>, not <code>ReLU()</code> as an activation function. This is because sparsity (many, many zeros for negative values that <code>ReLU()</code> provides) is an undesirable trait in a GAN&ndash; less non-zero weights means less levers the Generator can pull to fool the Discriminator.</p></li>
</ul>

<h4 id="conv2dtranspose">Conv2DTranspose</h4>

<p>This one threw me for a loop.</p>

<p>The basic idea of the <code>Conv2DTranspose()</code> layer is for upsampling our small, random images into something the same size as our real data, <code>X</code>.</p>

<p>Particularly, <a href="https://www.youtube.com/watch?v=ByjaPdWXKJ4&amp;feature=youtu.be&amp;t=16m59s">this YouTube video</a> provided a nice visual representation of what the <em>reverse</em> of a Convolution operation accomplishes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/conv2dtranspose.PNG&#39;</span><span class="p">)</span></code></pre></div>
<p><img src="vanilla_gan_7_0.png" alt="png" /></p>

<p>Moreover, <a href="https://github.com/vdumoulin/conv_arithmetic#transposed-convolution-animations">this repo</a> has some great graphics to cement the intuition for Transpose Convolution&ndash; particularly when it comes to understanding what &ldquo;stride&rdquo; means in this context.</p>

<p><strong>Note:</strong> Setting <code>strides=2</code> means that you insert a blank space between each pixel in the original image. This (with <code>padding=same</code>) is how we achieve upsampling from <code>16</code> to <code>32</code>.</p>

<p><strong>One more note:</strong> Chollet has a great nugget of wisdom with respect to <code>kernel_size</code> vs <code>strides</code> for GANs</p>

<blockquote>
<p>In generated images, it is common to see &ldquo;checkerboard artifacts&rdquo; caused by unequal coverage of the pixel space in the generator. To fix this, we use a kernel size that is divisible by the stride size, whenever we use a strided Conv2DTranpose or Conv2D in both the generator and discriminator.</p>
</blockquote>

<h3 id="building-the-discriminator">Building the Discriminator</h3>

<p>Pretty straight-forward.</p>

<p>As with the Discriminator, we avoid sparsity by utilizing <code>LeakyReLU()</code> layers. We also introduce randomness to the learning by tossing a healthy amount of <code>Dropout()</code> before the final <code>Dense()</code> layer, used to do the <code>0/1</code> classification.</p>

<h3 id="building-the-gan">Building the GAN</h3>

<p>One tricky thing to note is the order that we compile our <code>Model()</code> objects.</p>

<p>The general pseudocode of training is:</p>

<ul>
<li>Train the Discriminator by itself</li>
<li>Pass those gradients back to the Generator</li>
<li>Train the Generator</li>
</ul>

<p>So you&rsquo;ll notice that when constructing the Networks that we use to construct the GAN, <strong>he compiles the Discriminator, but not the Generator</strong>.</p>

<p>This is because the <code>model.compile()</code> step <em>freezes the Network definition (particularly all of the <code>layer.trainable</code> values)</em> as-is.</p>

<p><strong>Only after the Discriminator is frozen</strong> do we turn around and say <code>discriminator.trainable = False</code>. Then we build the GAN by piping the noise into the Generator and the Generator into the Discriminator. And after we specify that the overall model takes noise from the latent space and outputs the prediction on an image do we compile the network in its entirety.</p>

<p>This compiled object will have both models connected and talking to one another via gradients, but only the <code>Generator</code> parameters will be trainable.</p>

<h3 id="training">Training</h3>

<p>He does a lot of data prep here, particularly regarding:</p>

<ul>
<li>Loading the dataset</li>
<li>Scaling</li>
<li>Handling batch size</li>
</ul>

<p>That&rsquo;s all pretty straight-forward and not specific to GANs</p>

<h4 id="train-the-discriminator">Train the Discriminator</h4>

<p>Before we can train our discrimator to sort real from fake images, we need some fake images, so we:</p>

<ul>
<li>Sample random noise from the latent space</li>
<li>Use our Generator to make fake images via <code>generator.predict()</code></li>
</ul>

<p>Where we&rsquo;ll diverge (and most example I&rsquo;ve seen do&hellip;) is <strong>not</strong> concatenating the real and fake images into one list. This allows us to better-monitor the loss and accuracy for both Networks over both groups.</p>

<p>Then we&rsquo;ll leverage the same &ldquo;multiply by <code>0.05 * normal_noise</code>&rdquo; trick that he uses. This dampens our Discriminator from learning TOO quickly.</p>

<p>Then we train the Discriminator with <code>discriminator.train_on_batch()</code> (which only goes through one training step)</p>

<h4 id="train-the-generator">Train the Generator</h4>

<p>Now, we&rsquo;re going to generate <strong>all</strong> fake images, lie and say they&rsquo;re real, and send them over to the Discriminator.</p>

<p>We use <code>gan.train_on_batch()</code>, passing in latent noise and a bunch of True&rsquo;s. Because the Generator is hooked into the Discriminator, all of the gradients from prediction error flow backward and inform the Generator. Recall that <strong>every single image sent over was fake, but we said it was true</strong>, let&rsquo;s consider both examples:</p>

<ul>
<li>The Discriminator says &ldquo;This is real&rdquo;: It matches the label we sent over, reinforcing this kind of image being sent in the future</li>
<li>The Discriminator says &ldquo;This is fake&rdquo;: Whatever the Generator thought it&rsquo;d learned about the underling data distribution <code>pr_data</code>, was clearly not representative of how we get our images. Try something else.</li>
</ul>

<p>In either case, this is a win-win for our Generator&hellip; assuming that there&rsquo;s some healthy mix of fooling and not-foolilng the Discriminator.</p>

<p>And therein lies the rub.</p>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/napsterinblue/notes/issues/new'>and submit a suggested change</a>. You can also message me directly on <a href='https://twitter.com/napsterinblue'>Twitter</a>.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 179 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
